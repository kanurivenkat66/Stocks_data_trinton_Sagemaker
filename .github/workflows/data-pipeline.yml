name: Data Pipeline

on:
  workflow_dispatch:
    inputs:
      num_samples:
        description: 'Number of transaction samples to generate'
        required: false
        default: '100000'
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: "us-west-2"
  PYTHON_VERSION: "3.11"

jobs:
  data-pipeline:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Get Data Bucket
        id: bucket
        run: |
          BUCKET_NAME=$(aws s3 ls | grep fraud-detection-data | awk '{print $3}')
          echo "bucket_name=$BUCKET_NAME" >> $GITHUB_OUTPUT
      
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('fraud-detection-system/data-pipeline/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          cd fraud-detection-system/data-pipeline
          pip install --upgrade pip
          pip install -r requirements.txt 2>/dev/null || pip install pandas numpy boto3
      
      - name: Generate Sample Data
        id: generate
        run: |
          cd fraud-detection-system/data-pipeline
          python generate_sample_data.py \
            --num-samples ${{ github.event.inputs.num_samples || '100000' }} \
            --output-path s3://${{ steps.bucket.outputs.bucket_name }}/raw-transactions/ \
            --seed 42
          echo "status=completed" >> $GITHUB_OUTPUT
      
      - name: Data Preprocessing
        id: preprocess
        run: |
          cd fraud-detection-system/data-pipeline
          python data_preprocessing.py \
            --input-path s3://${{ steps.bucket.outputs.bucket_name }}/raw-transactions/transactions.csv \
            --s3-bucket ${{ steps.bucket.outputs.bucket_name }} \
            --test-split 0.15 \
            --val-split 0.15
      
      - name: Data Quality Report
        id: quality
        run: |
          cd fraud-detection-system/data-pipeline
          python -c "
          import pandas as pd
          import boto3
          
          s3 = boto3.client('s3')
          
          # Check data exists
          paginator = s3.get_paginator('list_objects_v2')
          pages = paginator.paginate(Bucket='${{ steps.bucket.outputs.bucket_name }}', Prefix='training-data/')
          
          total_files = 0
          for page in pages:
              if 'Contents' in page:
                  total_files += len(page['Contents'])
          
          print(f'✓ Training data files created: {total_files}')
          " || echo "Warning: Could not verify data quality"
      
      - name: Upload Data Report
        uses: actions/upload-artifact@v3
        with:
          name: data-quality-report
          path: fraud-detection-system/data-pipeline/
          retention-days: 30
      
      - name: Slack Notification
        if: always()
        uses: slackapi/slack-github-action@v1.24.0
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          payload: |
            {
              "text": "${{ job.status == 'success' && '✅' || '❌' }} Data Pipeline",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "${{ job.status == 'success' && '✅' || '❌' }} *Data Pipeline ${{ job.status }}*\n*Samples*: ${{ github.event.inputs.num_samples || '100000' }}\n*Bucket*: ${{ steps.bucket.outputs.bucket_name }}"
                  }
                }
              ]
            }
        continue-on-error: true
