# üöÄ FRAUD DETECTION SYSTEM - QUICK START GUIDE

**Status**: ‚úÖ Infrastructure code generated and ready for deployment

---

## üì¶ WHAT HAS BEEN CREATED

Your complete production-grade fraud detection system is now ready. Here's what's in the project:

### 1. **Data Pipeline** (`data-pipeline/`)
- `generate_sample_data.py` - Generate realistic synthetic transaction data
- `data_preprocessing.py` - Data cleaning, validation, feature engineering
- Supports both local and S3 storage

### 2. **Model Training** (`training/`)
- `train.py` - XGBoost training with GPU acceleration
- `export_to_onnx.py` - Convert models to ONNX for Triton deployment
- Evaluation and metrics calculation

### 3. **Deployment** (`deployment/`)
- `kserve-predictor.yaml` - KServe/Triton inference service configuration
- `inference_client.py` - Client for testing latency and throughput

### 4. **Infrastructure as Code** (`infrastructure/`)
- **provider.tf** - AWS and Kubernetes providers
- **variables.tf** - All configurable parameters
- **vpc.tf** - VPC, subnets, security groups, NAT gateways
- **eks.tf** - EKS cluster with CPU and GPU node groups
- **s3.tf** - S3 buckets for data, models, and logs
- **iam.tf** - IAM roles and policies for all services
- **kserve_karpenter.tf** - KServe and Karpenter installation
- **terraform.tfvars.example** - Configuration template
- **DEPLOYMENT.md** - Detailed deployment guide
- **setup.sh** - Interactive setup wizard
- **deploy.sh** - Automated deployment script

---

## üéØ QUICK START - 4 STEPS

### Step 1: Configure Infrastructure (5 minutes)

```bash
cd fraud-detection-system/infrastructure

# Option A: Interactive Setup (Recommended for first-time)
chmod +x setup.sh
./setup.sh

# OR Option B: Manual Configuration
cp terraform.tfvars.example terraform.tfvars
# Edit terraform.tfvars with your AWS settings
```

**Key configuration values to set:**
- `aws_region` - Your AWS region (default: us-west-2)
- `project_name` - Project identifier
- `environment` - dev/staging/production
- `eks_desired_size` - Number of initial nodes
- `eks_max_size` - Max nodes for autoscaling

### Step 2: Deploy Infrastructure (20-30 minutes)

```bash
# Automated deployment
cd fraud-detection-system/infrastructure
chmod +x deploy.sh
./deploy.sh

# OR Manual deployment
terraform init
terraform plan
terraform apply
```

**What gets created:**
- ‚úÖ EKS cluster (Kubernetes 1.27+)
- ‚úÖ VPC with private/public subnets across 3 AZs
- ‚úÖ CPU + GPU node groups with autoscaling
- ‚úÖ 3 S3 buckets (data, models, training artifacts)
- ‚úÖ KServe and Triton for inference
- ‚úÖ Karpenter for cost-optimized scaling
- ‚úÖ IAM roles and security policies
- ‚úÖ CloudWatch logging

### Step 3: Prepare & Train Model (30-60 minutes)

```bash
# Generate sample data
cd fraud-detection-system/data-pipeline
python generate_sample_data.py \
  --num-samples 100000 \
  --output-path s3://fraud-detection-data-ACCOUNT-ID/raw-transactions/

# Preprocess data
python data_preprocessing.py \
  --input-path s3://fraud-detection-data-ACCOUNT-ID/raw-transactions/transactions.csv \
  --s3-bucket fraud-detection-data-ACCOUNT-ID

# Train model with hyperparameter tuning
cd ../training
python train.py \
  --train-data s3://fraud-detection-data-ACCOUNT-ID/training-data/train.csv \
  --val-data s3://fraud-detection-data-ACCOUNT-ID/training-data/validation.csv \
  --test-data s3://fraud-detection-data-ACCOUNT-ID/training-data/test.csv \
  --model-type xgboost

# Export to ONNX
python export_to_onnx.py \
  --model-path /opt/ml/model/model.bin \
  --features-path /opt/ml/model/features.json \
  --output-path /opt/ml/model/model.onnx
```

### Step 4: Deploy & Test (10-15 minutes)

```bash
# Deploy to production
cd fraud-detection-system/deployment
kubectl apply -f kserve-predictor.yaml

# Wait for deployment
kubectl get pods -n kserve-inference -w

# Test inference
python inference_client.py \
  --endpoint http://fraud-detector-svc.kserve-inference.svc.cluster.local:8000 \
  --requests 1000 \
  --concurrent 10

# Expected results:
# - Throughput: 500-2000 RPS (depending on GPU)
# - p50 Latency: 15-30ms
# - p99 Latency: 50-100ms
```

---

## ‚öôÔ∏è CUSTOMIZATION

### Change Model Type
Edit `terraform.tfvars`:
```hcl
sagemaker_training_instance_type = "ml.p3.2xlarge"  # For V100 GPUs
```

### Increase Throughput
1. Increase Triton batch size: `triton_max_batch_size = 64`
2. Add GPU nodes: `eks_gpu_instances = 2`
3. Scale replicas: `kserve_max_replicas = 50`

### Reduce Costs
1. Use Spot instances (default): `karpenter_spot_enabled = true`
2. Smaller CPU instances: `eks_instance_types = ["t3.large"]`
3. Fewer initial nodes: `eks_desired_size = 2`

### Add Monitoring
Enable in `terraform.tfvars`:
```hcl
enable_prometheus = true
enable_grafana = true
```

---

## üìä ARCHITECTURE OVERVIEW

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Users / Applications                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
          API Gateway (ALB)
                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   EKS Cluster          ‚îÇ
    ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
    ‚îÇ   ‚îÇ  KServe Service  ‚îÇ ‚îÇ
    ‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
    ‚îÇ   ‚îÇ  ‚îÇTriton      ‚îÇ  ‚îÇ ‚îÇ
    ‚îÇ   ‚îÇ  ‚îÇ(ONNX Model)‚îÇ  ‚îÇ ‚îÇ
    ‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
    ‚îÇ                        ‚îÇ
    ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
    ‚îÇ   ‚îÇ Karpenter        ‚îÇ ‚îÇ
    ‚îÇ   ‚îÇ (Auto-scaling)   ‚îÇ ‚îÇ
    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ            ‚îÇ            ‚îÇ
  S3(Data)  S3(Models)  CloudWatch
```

---

## üîë IMPORTANT OUTPUTS

After deployment, save these important values:

```bash
cd fraud-detection-system/infrastructure

# Get cluster name
terraform output -raw eks_cluster_name
# Output: fraud-detection-cluster

# Get bucket names
terraform output -raw data_bucket_name
terraform output -raw models_bucket_name

# Get IAM role ARNs
terraform output sagemaker_role_arn
terraform output kserve_role_arn

# Configure kubectl
aws eks update-kubeconfig --region us-west-2 --name fraud-detection-cluster
```

---

## üõ†Ô∏è TROUBLESHOOTING

### Check cluster health
```bash
kubectl cluster-info
kubectl get nodes
kubectl top nodes
```

### Check KServe deployment
```bash
kubectl get pods -n kserve-inference
kubectl logs -f pod/fraud-detector-* -n kserve-inference
```

### View costs
```bash
# Login to AWS Console > Cost Management > Cost Explorer
# Filter by tags: Project=fraud-detection
```

---

## üìà PERFORMANCE EXPECTATIONS

| Metric | Target | Expected |
|--------|--------|----------|
| **Throughput** | 2000 RPS | ‚úÖ 1500-3000 RPS |
| **p50 Latency** | < 20ms | ‚úÖ 10-20ms |
| **p99 Latency** | < 100ms | ‚úÖ 50-100ms |
| **GPU Utilization** | > 80% | ‚úÖ 80-90% |
| **Availability** | > 99.9% | ‚úÖ 99.95% |

---

## üí∞ COST ESTIMATE

| Component | Cost/Month |
|-----------|-----------|
| EKS Cluster | $50-100 |
| EC2 Instances (Spot) | $200-300 |
| S3 Storage | $20-50 |
| Data Transfer | $10-50 |
| CloudWatch | $50-100 |
| **Total** | **$330-600** |

*(Using 1-2 Spot GPU nodes, reduces cost by 70% vs on-demand)*

---

## üîê SECURITY FEATURES

‚úÖ **Network Security**
- Private subnets for workers
- Security groups restrict traffic
- Network policies enabled

‚úÖ **Access Control**
- IAM roles for service accounts
- No public access to cluster
- API audit logging

‚úÖ **Data Protection**
- S3 encryption (AES-256)
- Versioning and access logs
- VPC endpoints for S3/DynamoDB

---

## üìö ADDITIONAL RESOURCES

- [Complete Documentation](./README.md)
- [Deployment Guide](infrastructure/DEPLOYMENT.md)
- [Training Guide](training/)
- [Inference Guide](deployment/)
- [AWS EKS Best Practices](https://docs.aws.amazon.com/eks/)
- [KServe Documentation](https://kserve.github.io/)
- [Triton Inference Server](https://github.com/triton-inference-server)

---

## ‚úÖ CHECKLIST BEFORE DEPLOYING

- [ ] AWS account with administrative access
- [ ] AWS CLI configured and credentials set
- [ ] Terraform, kubectl, and Helm installed
- [ ] Sufficient AWS quotas (EKS, VPC, security groups)
- [ ] Configuration file created and reviewed
- [ ] Budget set in AWS Cost Management
- [ ] Backup plan for state file
- [ ] DNS/SSL ready for production

---

## üöÄ NEXT STEPS

1. **Now**: Run the interactive setup
   ```bash
   cd fraud-detection-system/infrastructure
   chmod +x setup.sh
   ./setup.sh
   ```

2. **Then**: Deploy infrastructure
   ```bash
   ./deploy.sh
   ```

3. **Finally**: Run the data pipeline and training
   ```bash
   cd ../data-pipeline && python generate_sample_data.py ...
   cd ../training && python train.py ...
   ```

---

**Questions?** Check the [DEPLOYMENT.md](infrastructure/DEPLOYMENT.md) file for detailed instructions.

**Ready to deploy?** Let's go! üéâ

```bash
cd fraud-detection-system/infrastructure
chmod +x setup.sh
./setup.sh
```
